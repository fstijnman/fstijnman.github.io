---
layout: post
title: "Week 08"
date: 2025-02-22
categories: [weekly]
tags: [LLM, GenAI]
---

# Week 08

Hey there, here's a list of things I read in the past week.

1. TOC
{:toc}

### [Generative AI Meets Open-Ended Survey Responses: Participant Use of AI and Homogenization](https://www.gsb.stanford.edu/faculty-research/working-papers/generative-ai-meets-open-ended-survey-responses-participant-use-ai)

Read on the usage of LLMs by survey participants. Shows one of the effects of LLMs. Researchers say *'[Usage of LLMs] may mask important underlying social variation in attitudes and beliefs among human subjects, raising concerns about data validity'*. It reminds me of the "model collapse"-theory. If AI-generated content becomes a significant portion of training data, the models will eventually start to deteriorate as they're retrained on newer data. Curious to see how this will unfold in the future.

---

### [No AI December Reflections](https://blog.rybarix.com/2025/02/09/noaidecember.html)

Blog of a developer that wanted to stop for a month using LLMs for coding. Almost like a dry January.

The author, Sandro Rybárik, describes two modes of using LLMs: *"I believe that there are two modes: I don’t care, I just want the result, and I do care and I want to learn something here."*

I recognize the struggle in my own daily work. It's so convenient to just ask away any problem you have. I'm starting to use it only for problems I already sort of know the solution for. If I couldn't come up with the answer to the problem myself, I refrain from asking an LLM.

---

### [Deep Research, information vs. insight, and the nature of science](https://www.interconnects.ai/p/deep-research-information-vs-insight-in-science)

Blog about Deep Research functionality of LLMs. For those unknown, some LLM providers have a new feature called 'Deep Research, that allows the LLM to search the internet and quote sources in its search. RAG on steroids so to speak.

I feel the blog, as many other blogs about LLMs, is quite speculative. Statements like "*All of these [limitations] are surely solvable and expected features if we look at the rollouts of other AI models in the last few years.*"

I see these type of quotes often and think, well, I still have to see it. A nice demo doesn't imply it works that well in the real-world.

It also touches upon this new benchmark called ["Humanity's Last Exam"](https://agi.safe.ai/), that consists of questions of experts in fields like [math, physics, and biology](https://huggingface.co/datasets/cais/hle).

I mean it's cool if an AI can pass those tests. But does it actually prove anything? Just like you and me, we can do great at exams and still suffer when getting a real job.

Additionally, I was reading this [NYT article](https://www.nytimes.com/2025/01/23/technology/ai-test-humanitys-last-exam.html) on LLM evaluations:

*"Part of what’s so confusing about A.I. progress these days is how jagged it is. We have A.I. models capable of diagnosing diseases more effectively than human doctors, winning silver medals at the International Math Olympiad and beating top human programmers on competitive coding challenges.
But these same models sometimes struggle with basic tasks, like arithmetic or writing metered poetry."*

---

### [Fine-tune any LLM in four simple steps](https://github.com/PacktPublishing/LLM-Engineers-Handbook)

From Paul Iusztin's LLM Engineers Handbook:

![Fine tune LLMs](/assets/fine-tune-llms.png)

Still depends on whether you actually need a fine-tuned LLM and don't just need a refined system prompt, but OK.

---

`Link template`

> [Title of Link](https://example.com)

<!-- Optional: Add image below -->
<!-- ![Image Description](/images/logo.png) -->

Your commentary goes here. This is where you can add your thoughts, analysis, or reactions to the linked content.

---
