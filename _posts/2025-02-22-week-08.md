---
layout: post
title: "Week 08"
date: 2025-02-22
categories: [weekly]
tags: [LLM, GenAI]
---

# Week 08

Hey there, here's a list of things I read in the past week.

1. TOC
{:toc}

### [Generative AI Meets Open-Ended Survey Responses: Participant Use of AI and Homogenization](https://www.gsb.stanford.edu/faculty-research/working-papers/generative-ai-meets-open-ended-survey-responses-participant-use-ai)

Read on the usage of LLMs by survey participants. Shows one of the effects of LLMs. Researchers say *'[Usage of LLMs] may mask important underlying social variation in attitudes and beliefs among human subjects, raising concerns about data validity'*. It reminds me of the "model collapse"-theory. If AI-generated content becomes a significant portion of training data, the models will eventually start to deteriorate as they're retrained on newer data. Curious to see how this will unfold in the future.

---

### [No AI December Reflections](https://blog.rybarix.com/2025/02/09/noaidecember.html)

Blog of a developer that wanted to stop for a month using LLMs for coding. Almost like a dry January.

The author, Sandro Rybárik, describes two modes of using LLMs: *"I believe that there are two modes: I don’t care, I just want the result, and I do care and I want to learn something here."*

I recognize the struggle in my own daily work. It's so convenient to just ask away any problem you have. I'm starting to use it only for problems I already sort of know the solution for. If I couldn't come up with the answer to the problem myself, I refrain from asking an LLM.

---

### [Deep Research, information vs. insight, and the nature of science](https://www.interconnects.ai/p/deep-research-information-vs-insight-in-science)

Blog about Deep Research functionality of LLMs. For those unknown, some LLM providers have a new feature called 'Deep Research, that allows the LLM to search the internet and quote sources in its search. RAG on steroids so to speak.

I feel the blog, as many other blogs about LLMs, is quite speculative. Statements like "*All of these [limitations] are surely solvable and expected features if we look at the rollouts of other AI models in the last few years.*"

I see these type of quotes often and think, well, I still have to see it. A nice demo doesn't imply it works that well in the real-world.

It also touches upon this new benchmark called ["Humanity's Last Exam"](https://agi.safe.ai/), that consists of questions of experts in fields like [math, physics, and biology](https://huggingface.co/datasets/cais/hle).

I mean it's cool if an AI can pass those tests. But does it actually prove anything? Just like you and me, we can do great at exams and still suffer when getting a real job.

Additionally, I was reading this [NYT article](https://www.nytimes.com/2025/01/23/technology/ai-test-humanitys-last-exam.html) on LLM evaluations:

*"Part of what’s so confusing about A.I. progress these days is how jagged it is. We have A.I. models capable of diagnosing diseases more effectively than human doctors, winning silver medals at the International Math Olympiad and beating top human programmers on competitive coding challenges.
But these same models sometimes struggle with basic tasks, like arithmetic or writing metered poetry."*

---

### [Fine-tune any LLM in four simple steps](https://github.com/PacktPublishing/LLM-Engineers-Handbook)

From Paul Iusztin's LLM Engineers Handbook:

![Fine tune LLMs](/assets/fine-tune-llms.png)

Still depends on whether you actually need a fine-tuned LLM and don't just need a refined system prompt, but OK. The Handbook itself is a great read.

---

### [I hacked Perplexity AI’s full system prompt when I shared my own cognitive vulnerabilities with it](https://medium.com/the-generator/prompt-hacking-perplexity-ai-system-instructions-7aa6ee923060)

<!-- Optional: Add image below -->
<!-- ![Image Description](/images/logo.png) -->

An AI prompt injection at Perplexity to retrieve its system prompt. It's fun to see what's behind a system like that as guardrails.

---

### [Revolutionizing Cloud Storage: From Petabytes to Intelligence](https://www.youtube.com/watch?v=cEcGoJrTptA)

<!-- Optional: Add image below -->
<!-- ![Image Description](/images/logo.png) -->

Talk about DocuSign's implementation of 'agentic AI'. Learnings:

Explains the importance of end-to-end visibility in agentic systems.

Gives an MLOps quick rubric for agentic AI:
  1. *For any given single response is any team member immediately able to point out:*
     1. environments (libraries, OS, etc),
     2. system logs (tracebacks, memory usage, response times),
     3. model versioning (base model, LoRa),
     4. prompt (or all prompts that happened throughout the system),
     5. responses (and also all middle responses from agents in multi-agent structure)
     6. inference code
     7. RAG code
     8. generated RAG context
     9. testing/evals results for model
  2.  *For all fine tuned models:*
      1.  Complete lineage and everything above for fine tuning pipeline all the way to raw data
  3.  *For RAG*
      1.  Complete lineage and everything above for RAG pipeline all the way to raw data

Shows the complexity of using a system like this.


---

### [Temporal quality degradation in AI models](https://www.nature.com/articles/s41598-022-15245-z)

<!-- Optional: Add image below -->
<!-- ![Image Description](/images/logo.png) -->

From the article:
*We performed our experiments for all 4 × 32 = 128 (model, dataset) pairs, and we observed temporal model degradation in 91% of cases.*

This highlights what we're going to see with LLMs as well. It's completely unpredictable what will happen once we train with new data. And we need to train with new data. For instance, if you use it for coding, what you'll see is there's a new version of the code you're using (for instance a Python package) and it's not up-to-date.

And since most of the business using LLMs are dependent on companies to put out a new model, degradation is going to have a much larger impact than it did before.
